{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'X', 'S', 'B', 'M', 'Q', 'I', '-', 'L', 'T', 'K', 'W', 'E', 'V', 'G', 'F', 'R', 'P', 'C', 'H', 'N', 'Y', 'D', 'H2A', 'H4', 'H1', 'H3', 'H2B']\n"
     ]
    }
   ],
   "source": [
    "vocab = SeqIO.to_dict(SeqIO.parse('sequences.fasta', 'fasta'))\n",
    "\n",
    "uniq_seq = list(map(lambda x: str(x.seq), vocab.values()))\n",
    "uniq_seq = ''.join(uniq_seq)\n",
    "\n",
    "uniq_hist = list(map(lambda x: x.split('|')[-2], vocab.keys()))\n",
    "\n",
    "vocab = list(set(uniq_seq)) + list(set(uniq_hist))\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'X': 1, 'S': 2, 'B': 3, 'M': 4, 'Q': 5, 'I': 6, '-': 7, 'L': 8, 'T': 9, 'K': 10, 'W': 11, 'E': 12, 'V': 13, 'G': 14, 'F': 15, 'R': 16, 'P': 17, 'C': 18, 'H': 19, 'N': 20, 'Y': 21, 'D': 22, 'H2A': 23, 'H4': 24, 'H1': 25, 'H3': 26, 'H2B': 27}\n",
      "{0: 'A', 1: 'X', 2: 'S', 3: 'B', 4: 'M', 5: 'Q', 6: 'I', 7: '-', 8: 'L', 9: 'T', 10: 'K', 11: 'W', 12: 'E', 13: 'V', 14: 'G', 15: 'F', 16: 'R', 17: 'P', 18: 'C', 19: 'H', 20: 'N', 21: 'Y', 22: 'D', 23: 'H2A', 24: 'H4', 25: 'H1', 26: 'H3', 27: 'H2B'}\n"
     ]
    }
   ],
   "source": [
    "nums = list(range(29))\n",
    "dict_1 = dict(zip(vocab, nums))\n",
    "dict_2 = dict(zip(nums, vocab))\n",
    "\n",
    "print(dict_1)\n",
    "print(dict_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565\n",
      "565\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "sequences = SeqIO.to_dict(SeqIO.parse('sequences.fasta', 'fasta'))\n",
    "sequences_val = list(map(lambda y: str(y.seq), sequences.values()))\n",
    "\n",
    "seq_tensors = list(map(lambda x: list(map(lambda k: dict_1[k], x)), sequences_val))\n",
    "hist_tensors = list(map(lambda x: dict_1[x.split('|')[-2]], sequences))\n",
    "print(len(seq_tensors))\n",
    "print(len(hist_tensors))\n",
    "print(hist_tensors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте два словаря: в первом словаре ключом является символ, а значением его порядковый номер в словаре vocab; во втором словаре - наоборот. \n",
    "\n",
    "Первый словарь понадобится для того, чтобы закодировать последовательность и целевые значения (тип гистона) в виде тензора. \n",
    "\n",
    "Второй словарь, чтобы “превратить” тензор (например, полученный в результате предсказания) с исходный символ или тип гистона. \n",
    "\n",
    "Используя первый словарь, получите список тензоров, в котором каждый тензор - это закодированная последовательность, и тензор, хранящий закодированные целевые значения (типы гистонов для каждой последовательности). \n",
    "\n",
    "Сколько последовательностей получилось в предобработанном датасете (длина списка тензоров)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3776/3981069463.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  seq_tensors = torch.tensor(seq_tensors)\n",
      "/tmp/ipykernel_3776/3981069463.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hist_tensors = torch.tensor(hist_tensors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 1.2587\n",
      "Epoch 100, Loss: 0.3835\n",
      "Epoch 150, Loss: 0.1898\n",
      "Epoch 200, Loss: 0.0712\n",
      "Epoch 250, Loss: 0.0827\n",
      "Epoch 300, Loss: 0.0410\n",
      "Epoch 350, Loss: 0.0241\n",
      "Epoch 400, Loss: 0.0331\n",
      "Epoch 450, Loss: 0.0457\n",
      "Epoch 500, Loss: 0.0218\n",
      "Epoch 550, Loss: 0.0133\n",
      "Epoch 600, Loss: 0.0105\n",
      "Epoch 650, Loss: 0.0049\n",
      "Epoch 700, Loss: 0.0027\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n",
    "\n",
    "embedding_dim = 5\n",
    "hidden_dim = 9\n",
    "vocab_size = len(seq_tensors)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "seq_tensors = torch.tensor(seq_tensors)\n",
    "hist_tensors = torch.tensor(hist_tensors)\n",
    "\n",
    "class RNNWithAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=5, hidden_dim=9, random_seed=5):\n",
    "        super(RNNWithAttentionModel, self).__init__()\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        # RNN layer (LSTM)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Embed the input\n",
    "        x = self.embedding(x)  \n",
    "        \n",
    "        # RNN processing\n",
    "        rnn_out, _ = self.rnn(x)  \n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = self.attention(rnn_out)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        # Compute context vector (weighted sum)\n",
    "        context = torch.sum(attention_weights * rnn_out, dim=1)\n",
    "        \n",
    "        # Final output\n",
    "        out = self.fc(context)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Пример использования:\n",
    "test_sequence = [1, 5, 10, 15]  # или строка \"ACGT\"\n",
    "pred_class, class_probs = predict_single(model, test_sequence, vocab)\n",
    "print(f\"Предсказанный класс: {pred_class}\")\n",
    "print(f\"Вероятности классов: {class_probs}\")\n",
    "\n",
    "\n",
    "# Training setup\n",
    "vocab_size = len(seq_tensors)\n",
    "model = RNNWithAttentionModel(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, epochs=700):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # for i in range(len(X_train)) :\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch+1) % 50 == 0:\n",
    "            print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Example usage (assuming you have DataLoader prepared)\n",
    "# train_loader = ...  # Your DataLoader\n",
    "# train_model(model, train_loader)\n",
    "X_train, X_test, y_train, y_test = train_test_split(seq_tensors, hist_tensors, test_size=0.2, random_state=5) \n",
    "\n",
    "batch_s = 32\n",
    "\n",
    "train_ds = SequenceDataset(X_train, y_train)\n",
    "val_ds = SequenceDataset(X_test, y_test)\n",
    "\n",
    "train_l = DataLoader(train_ds, batch_size=batch_s, shuffle=True, num_workers=2)\n",
    "val_l = DataLoader(val_ds, batch_size=batch_s, shuffle=True, num_workers=2)\n",
    "\n",
    "train_model(model, train_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выполнения задания вы можете самостоятельно создать Python-ноутбук, а в LMS прикладывать лишь требуемые результаты. В этом задании вам необходимо написать рекуррентную нейросеть с механизмом внимания с использованием PyTorch. В качестве входного слоя создайте Embedding. Далее создайте 1 рекуррентный слой. В качестве слоя внимания используйте линейную трансформацию. В качестве выходного слоя используйте линейную трансформацию. Используйте код для решения данной задачи:\n",
    "\n",
    "Запустите обучение нейросети на 700 эпохах. Используйте значение кросс-энтропии в качестве функции потерь и адаптивный алгоритм Adam со скоростью обучения 0.01 в качестве алгоритма оптимизации. Выберете верно утверждение:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя обученную модель нейросети предскажите тип гистона для последовательности: \n",
    "\n",
    "seq_fasta = '''>Pan|XP_003311177.1|HTYPE|HVARIANT \n",
    "\n",
    "MSGRGKQGGKARTKAKTRSSRAGLQFPVGRVHRLLRKGNYAERVGAGAPVYLAAVLEYLT \n",
    "\n",
    "AEILELAGNAARDNKKTRIIPRHLQLAIRNDEELNKLLGKVTIAQGGVLPNIQAVLLPKK \n",
    "\n",
    "TESHHKAKGK'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказанный класс: 25\n",
      "Вероятности классов: [3.6703043e-09 2.9734539e-09 2.7026759e-09 3.3443814e-09 3.6848591e-09\n",
      " 1.7214317e-09 2.2935387e-09 1.7233273e-09 2.0864197e-09 1.5227409e-09\n",
      " 2.4711433e-09 1.8455484e-09 3.7518291e-09 2.3855673e-09 3.2569794e-09\n",
      " 2.1371651e-09 2.8205573e-09 1.8180104e-09 2.0338393e-09 1.4317328e-09\n",
      " 3.5843315e-09 2.3692881e-09 2.0004218e-09 1.9196827e-06 1.1646406e-10\n",
      " 9.9994230e-01 1.6399394e-05 3.8153339e-05 4.1861141e-09 3.3393650e-09\n",
      " 2.5912874e-09 1.6337061e-09 1.7432090e-09 2.5053803e-09 3.1800571e-09\n",
      " 2.9284661e-09 1.6818146e-09 1.8076030e-09 5.4480296e-09 2.3044573e-09\n",
      " 3.0361917e-09 2.6623332e-09 2.1679945e-09 1.8101768e-09 1.6033944e-09\n",
      " 3.3101628e-09 2.0156885e-09 1.9749793e-09 4.3199844e-09 2.8158540e-09\n",
      " 1.9166084e-09 1.8296501e-09 1.9162283e-09 3.0509482e-09 2.3431683e-09\n",
      " 2.5632483e-09 1.2803018e-09 2.5669273e-09 1.8733926e-09 5.3419420e-09\n",
      " 2.3240203e-09 3.8736871e-09 2.3721098e-09 2.0972601e-09 1.2146811e-09\n",
      " 3.3580474e-09 2.3361890e-09 1.2403502e-09 1.5411724e-09 2.0764668e-09\n",
      " 2.2845883e-09 2.4668532e-09 2.4946372e-09 2.6602673e-09 2.2287521e-09\n",
      " 1.6510385e-09 1.2420192e-09 2.9412011e-09 1.8731319e-09 2.1009154e-09\n",
      " 1.2727127e-09 1.5241763e-09 2.2452860e-09 1.8898909e-09 2.8760510e-09\n",
      " 1.0781716e-09 1.6493514e-09 3.6238856e-09 1.5997013e-09 2.1119317e-09\n",
      " 2.3235152e-09 3.8989039e-09 1.9546935e-09 2.1663988e-09 1.4347754e-09\n",
      " 1.8425798e-09 3.8462771e-09 2.5337432e-09 2.6319638e-09 3.1565919e-09\n",
      " 3.1365706e-09 2.0154425e-09 1.2383786e-09 3.0264957e-09 1.4154359e-09\n",
      " 1.5248161e-09 1.3970232e-09 4.6865445e-09 3.4243313e-09 2.6726055e-09\n",
      " 4.6878319e-09 2.2289945e-09 4.1524442e-09 1.6658296e-09 3.1472616e-09\n",
      " 3.3743104e-09 2.3565787e-09 2.9812397e-09 2.5227436e-09 1.2703796e-09\n",
      " 2.0839059e-09 1.5441030e-09 2.2030933e-09 3.5469268e-09 2.8166760e-09\n",
      " 2.4129971e-09 3.2184573e-09 3.0698082e-09 3.5847825e-09 1.7812766e-09\n",
      " 4.5008295e-09 3.6341161e-09 1.8969295e-09 3.0731240e-09 1.8069618e-09\n",
      " 2.8486771e-09 2.3864000e-09 1.5407374e-09 1.8863762e-09 2.3970610e-09\n",
      " 2.5654541e-09 3.3352083e-09 2.5159537e-09 1.9598865e-09 2.8620963e-09\n",
      " 1.9692505e-09 4.0357411e-09 1.4797583e-09 1.5711438e-09 3.4531047e-09\n",
      " 2.3124400e-09 2.3868598e-09 2.0863600e-09 2.3593536e-09 3.5943197e-09\n",
      " 2.2098183e-09 4.2372603e-09 1.8848152e-09 1.1622135e-09 2.5930476e-09\n",
      " 1.3693047e-09 3.3250649e-09 2.2918158e-09 2.0440363e-09 1.7457678e-09\n",
      " 3.4937222e-09 2.6889062e-09 2.3544759e-09 2.1609017e-09 1.1134916e-09\n",
      " 1.5821381e-09 1.8351722e-09 2.2012618e-09 2.2272604e-09 1.4254249e-09\n",
      " 2.2874924e-09 1.8409568e-09 1.8139371e-09 2.1554600e-09 1.8345073e-09\n",
      " 1.5790690e-09 2.6098583e-09 1.1829004e-09 2.0870525e-09 3.6006804e-09\n",
      " 2.1989959e-09 2.8569807e-09 2.5404987e-09 2.0652098e-09 8.5726337e-10\n",
      " 2.0645365e-09 1.3269326e-09 2.3236923e-09 1.8926469e-09 3.7417238e-09\n",
      " 3.2598382e-09 3.7354133e-09 2.0652766e-09 4.3068615e-09 1.2959817e-09\n",
      " 3.7589345e-09 1.6102533e-09 2.9534337e-09 2.5660170e-09 2.9165095e-09\n",
      " 2.6032163e-09 8.6968771e-10 2.3017559e-09 1.7469836e-09 1.9520185e-09\n",
      " 4.0424282e-09 3.0808651e-09 1.9417399e-09 1.7978825e-09 1.5184803e-09\n",
      " 2.3267526e-09 2.1106192e-09 2.8146729e-09 2.6308493e-09 2.1411228e-09\n",
      " 1.4616541e-09 2.9511980e-09 2.7201923e-09 2.4943660e-09 2.8039882e-09\n",
      " 3.3822682e-09 2.3316882e-09 3.3844625e-09 1.9193704e-09 2.2552520e-09\n",
      " 2.4564626e-09 3.4517285e-09 3.5687029e-09 2.7413127e-09 2.1701954e-09\n",
      " 3.2331993e-09 1.1541770e-09 4.0910852e-09 1.4627977e-09 3.1657144e-09\n",
      " 3.6770238e-09 2.8327691e-09 1.9963815e-09 2.0943900e-09 3.5846321e-09\n",
      " 1.3568230e-09 3.4131742e-09 1.7873821e-09 8.2355340e-09 2.6704043e-09\n",
      " 2.2246198e-09 1.5610148e-09 2.3004874e-09 2.9213756e-09 1.4279584e-09\n",
      " 2.3993938e-09 1.9185102e-09 1.5780814e-09 1.2686047e-09 2.0020672e-09\n",
      " 2.3382529e-09 2.8830041e-09 1.8111646e-09 2.1944790e-09 3.2825491e-09\n",
      " 2.5816280e-09 3.1427025e-09 2.4724400e-09 1.9377588e-09 1.7372974e-09\n",
      " 1.5479541e-09 2.1021260e-09 2.3099846e-09 1.8767472e-09 1.9165720e-09\n",
      " 2.7377700e-09 1.5874763e-09 2.0435373e-09 1.6414083e-09 1.8313259e-09\n",
      " 3.2529812e-09 2.6579798e-09 2.5981368e-09 2.5846778e-09 2.2174815e-09\n",
      " 3.6554302e-09 3.7853751e-09 1.5250312e-09 4.0149422e-09 3.5005525e-09\n",
      " 1.7618678e-09 3.5316094e-09 2.2887841e-09 1.2817115e-09 2.1842896e-09\n",
      " 2.4136968e-09 2.7333871e-09 3.0502558e-09 2.7736897e-09 2.4615936e-09\n",
      " 2.2455384e-09 3.1163552e-09 3.5289365e-09 3.8573855e-09 9.0063296e-10\n",
      " 2.8591938e-09 3.3090455e-09 3.4691665e-09 3.6560022e-09 3.6808341e-09\n",
      " 1.2670472e-09 2.0636506e-09 2.4427682e-09 2.1179223e-09 2.3818798e-09\n",
      " 1.6329490e-09 2.2956921e-09 3.0903171e-09 2.2597688e-09 3.0417970e-09\n",
      " 2.9233096e-09 2.8243474e-09 2.2641522e-09 2.4642104e-09 1.9610866e-09\n",
      " 3.2526213e-09 1.7241558e-09 1.6232944e-09 1.9149313e-09 1.6745167e-09\n",
      " 1.3763012e-09 2.1352704e-09 1.4829143e-09 1.6535913e-09 2.1006028e-09\n",
      " 3.8973060e-09 1.3414044e-09 1.6270172e-09 1.7612564e-09 1.3473764e-09\n",
      " 2.0698749e-09 3.1294476e-09 2.2668702e-09 1.4284024e-09 1.6641178e-09\n",
      " 1.4220440e-09 1.7123412e-09 3.8215893e-09 2.5220508e-09 2.5010261e-09\n",
      " 2.0096385e-09 2.5447469e-09 2.3350395e-09 3.5504260e-09 1.9185287e-09\n",
      " 2.5279843e-09 2.0901638e-09 2.5047353e-09 1.2805434e-09 1.5702271e-09\n",
      " 1.4038906e-09 2.4876952e-09 1.0969481e-09 1.4985441e-09 2.7120948e-09\n",
      " 2.0088260e-09 1.8666977e-09 2.3440625e-09 3.5137238e-09 4.7952859e-09\n",
      " 2.6747065e-09 2.6000304e-09 4.4527559e-09 2.9798639e-09 1.9282762e-09\n",
      " 1.4957513e-09 1.5879185e-09 3.9722936e-09 1.3629988e-09 2.0774571e-09\n",
      " 1.1479965e-09 3.4344962e-09 1.7150181e-09 3.0366261e-09 1.0083225e-09\n",
      " 2.6230731e-09 1.5844877e-09 2.9447655e-09 1.7549052e-09 2.6565408e-09\n",
      " 3.0773180e-09 3.8945562e-09 2.0611683e-09 4.2532036e-09 2.6709188e-09\n",
      " 1.8382130e-09 1.9603390e-09 5.2698872e-09 1.6155168e-09 1.6489897e-09\n",
      " 1.9191948e-09 1.7221542e-09 1.0146853e-09 3.3595402e-09 3.7106587e-09\n",
      " 1.9542088e-09 1.6299867e-09 2.3754241e-09 4.4271644e-09 1.1594525e-09\n",
      " 2.9293770e-09 3.1789775e-09 2.0417490e-09 2.0968960e-09 2.4786204e-09\n",
      " 2.4094557e-09 2.1167672e-09 2.1370592e-09 1.1694671e-09 2.6066049e-09\n",
      " 1.6680584e-09 2.2320066e-09 1.7492909e-09 2.0384492e-09 2.5701266e-09\n",
      " 4.9978297e-09 1.6516873e-09 1.2718075e-09 2.1027837e-09 2.5044677e-09\n",
      " 1.8214361e-09 2.2371893e-09 1.5180140e-09 1.7792969e-09 3.4657008e-09\n",
      " 3.3998935e-09 4.1509636e-09 1.6328338e-09 2.0658362e-09 1.4300080e-09\n",
      " 4.5860911e-09 1.6116698e-09 1.7486237e-09 3.2684235e-09 1.7499550e-09\n",
      " 1.7772991e-09 3.4095111e-09 1.9195683e-09 3.5153527e-09 1.4167107e-09\n",
      " 2.3842752e-09 4.5785398e-09 1.8414028e-09 1.3821095e-09 1.6277342e-09\n",
      " 4.2892285e-09 2.2193729e-09 8.9479935e-10 2.8773017e-09 1.4507636e-09\n",
      " 2.1159234e-09 1.9088233e-09 1.0706078e-09 3.4206886e-09 1.8955335e-09\n",
      " 2.0542490e-09 1.8242489e-09 3.4144310e-09 4.0395300e-09 1.3633966e-09\n",
      " 2.9857579e-09 4.0525721e-09 3.1439378e-09 1.6554121e-09 3.6924928e-09\n",
      " 1.6787187e-09 2.8154621e-09 3.8675370e-09 2.7454308e-09 3.0999219e-09\n",
      " 2.1660187e-09 3.7059056e-09 5.1400297e-09 1.6375840e-09 2.6814442e-09\n",
      " 2.0555269e-09 2.6958644e-09 1.8295837e-09 1.6961532e-09 3.1741123e-09\n",
      " 3.2023284e-09 2.2222788e-09 2.5799940e-09 1.5606277e-09 3.1234721e-09\n",
      " 2.6292544e-09 3.0062670e-09 1.3877280e-09 2.9538392e-09 8.8823399e-10\n",
      " 2.5145914e-09 2.4798927e-09 1.9009607e-09 2.1411433e-09 2.7927420e-09\n",
      " 2.2878806e-09 3.2431431e-09 1.9017514e-09 1.7091044e-09 3.8183328e-09\n",
      " 2.0282882e-09 1.7679406e-09 4.4023660e-09 2.5736142e-09 3.6242449e-09\n",
      " 2.2262709e-09 3.0400162e-09 1.0809822e-09 1.1623488e-09 2.8363483e-09\n",
      " 2.9782730e-09 3.4879162e-09 1.9832691e-09 1.8660677e-09 2.9688920e-09\n",
      " 2.1104420e-09 1.4730113e-09 3.7177217e-09 2.2447977e-09 1.3831749e-09\n",
      " 1.5467825e-09 2.4590878e-09 1.2239535e-09 4.2057700e-09 3.0611609e-09\n",
      " 3.5370902e-09 1.4915577e-09 1.7751004e-09 1.6638259e-09 1.9121758e-09\n",
      " 1.8268465e-09 2.5055618e-09 3.5531020e-09 2.2795308e-09 1.8486134e-09\n",
      " 3.1645795e-09 2.1096129e-09 1.9511177e-09 2.4568187e-09 2.2031854e-09\n",
      " 3.5720740e-09 3.7851731e-09 1.6584334e-09 2.1248818e-09 1.3178852e-09\n",
      " 4.6116693e-09 1.2578563e-09 3.3944829e-09 1.0927215e-09 1.0571001e-09\n",
      " 1.8169082e-09 2.2223592e-09 2.9623362e-09 1.4353475e-09 2.1282849e-09]\n"
     ]
    }
   ],
   "source": [
    "def predict_single(model, sequence, vocab, max_len=50, device='cpu'):\n",
    "    \"\"\"\n",
    "    Предсказание для одной последовательности\n",
    "    \n",
    "    Args:\n",
    "        model: обученная модель\n",
    "        sequence: входная последовательность (список индексов)\n",
    "        vocab: словарь для преобразования токенов\n",
    "        max_len: максимальная длина последовательности\n",
    "        device: устройство для вычислений\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (предсказанный класс, вероятности классов)\n",
    "    \"\"\"\n",
    "    # Преобразуем последовательность в тензор\n",
    "    if isinstance(sequence, str):\n",
    "        # Если на входе строка, токенизируем ее\n",
    "        sequence = [vocab.get(token, 0) for token in sequence]  # 0 для OOV\n",
    "    \n",
    "    # Добавляем padding\n",
    "    padded_seq = torch.nn.functional.pad(\n",
    "        torch.tensor(sequence),\n",
    "        (0, max_len - len(sequence)),\n",
    "        value=0\n",
    "    )\n",
    "    \n",
    "    # Добавляем batch dimension\n",
    "    input_tensor = padded_seq.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Делаем предсказание\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "    \n",
    "    return pred, probs.squeeze().cpu().numpy()\n",
    "\n",
    "# Пример использования:\n",
    "test_sequence = 'MSGRGKQGGKARTKAKTRSSRAGLQFPVGRVHRLLRKGNYAERVGAGAPVYLAAVLEYLTAEILELAGNAARDNKKTRIIPRHLQLAIRNDEELNKLLGKVTIAQGGVLPNIQAVLLPKKTESHHKAKGK'  # или строка \"ACGT\"\n",
    "pred_class, class_probs = predict_single(model, test_sequence, dict_2)\n",
    "print(f\"Предсказанный класс: {pred_class}\")\n",
    "print(f\"Вероятности классов: {class_probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HomeWork_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
